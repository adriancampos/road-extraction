{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov  6 18:21:34 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   54C    P0    99W / 149W |   9115MiB / 11441MiB |     60%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           Off  | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   73C    P0    95W / 149W |   6011MiB / 11441MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla K80           Off  | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   49C    P0    58W / 149W |    672MiB / 11441MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla K80           Off  | 00000000:00:07.0 Off |                    0 |\n",
      "| N/A   74C    P0    92W / 149W |      0MiB / 11441MiB |     48%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0      7353      C   /opt/anaconda3/bin/python                   4212MiB |\n",
      "|    0     16848      C   /opt/anaconda3/bin/python                   4889MiB |\n",
      "|    1     26172      C   /opt/anaconda3/bin/python                   6000MiB |\n",
      "|    2     18096      C   /opt/anaconda3/bin/python                    661MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Currently running servers:\n",
      "http://localhost:8080/ :: /home/jupyter\n",
      "http://localhost:8080/ :: /home/jupyter\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!jupyter notebook list\n",
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import tiramisu\n",
    "from models import tiramisu_bilinear\n",
    "from models import tiramisu_m3\n",
    "from models import unet\n",
    "from datasets import deepglobe\n",
    "from datasets import maroads\n",
    "from datasets import joint_transforms\n",
    "import utils.imgs\n",
    "import utils.training as train_utils\n",
    "\n",
    "# tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Download the DeepGlobe dataset from https://competitions.codalab.org/competitions/18467. Place it in datasets/deepglobe/dataset/train,test,valid\n",
    "Download the Massachusetts Road Dataset from https://www.cs.toronto.edu/~vmnih/data/. Combine the training, validation, and test sets, process with `crop_dataset.ipynb` and place the output in datasets/maroads/dataset/map,sat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = \"expM.3.drop2.1\"\n",
    "DEEPGLOBE_PATH = Path('datasets/', 'deepglobe/dataset')\n",
    "MAROADS_PATH = Path('datasets/', 'maroads/dataset')\n",
    "RESULTS_PATH = Path('.results/')\n",
    "WEIGHTS_PATH = Path('.weights/')\n",
    "RUNS_PATH    = Path('.runs/')\n",
    "RESULTS_PATH.mkdir(exist_ok=True)\n",
    "WEIGHTS_PATH.mkdir(exist_ok=True)\n",
    "RUNS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "batch_size = 1 # TODO: Should be `MAX_BATCH_PER_CARD * torch.cuda.device_count()` (which in this case is 1 assuming max of 1 batch per card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# resize = joint_transforms.JointRandomCrop((300, 300))\n",
    "\n",
    "normalize = transforms.Normalize(mean=deepglobe.mean, std=deepglobe.std)\n",
    "train_joint_transformer = transforms.Compose([\n",
    "#     resize,\n",
    "    joint_transforms.JointRandomHorizontalFlip(),\n",
    "    joint_transforms.JointRandomVerticalFlip(),\n",
    "    joint_transforms.JointRandomRotate()\n",
    "    ])\n",
    "\n",
    "train_slice = slice(None,4000)\n",
    "test_slice = slice(4000,None)\n",
    "\n",
    "train_dset = deepglobe.DeepGlobe(DEEPGLOBE_PATH, 'train', slc = train_slice,\n",
    "    joint_transform=train_joint_transformer,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=.4,contrast=.4,saturation=.4),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "train_dset_ma = maroads.MARoads(MAROADS_PATH, \n",
    "    joint_transform=train_joint_transformer,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=.4,contrast=.4,saturation=.4),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "# print(len(train_dset_ma.imgs))\n",
    "# print(len(train_dset_ma.msks))\n",
    "train_dset_combine = torch.utils.data.ConcatDataset((train_dset, train_dset_ma))\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dset_ma, batch_size=batch_size, shuffle=True)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dset_combine, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# resize_joint_transformer = transforms.Compose([\n",
    "#     resize\n",
    "#     ])\n",
    "resize_joint_transformer = None\n",
    "val_dset = deepglobe.DeepGlobe(\n",
    "    DEEPGLOBE_PATH, 'valid', joint_transform=resize_joint_transformer,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]))\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dset = deepglobe.DeepGlobe(\n",
    "    DEEPGLOBE_PATH, 'train', joint_transform=resize_joint_transformer, slc = test_slice,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]))\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4909\n",
      "Val: 1243\n",
      "Test: 2226\n",
      "<torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x7f387e5c3dd0>\n",
      "Inputs:  torch.Size([1, 3, 1024, 1024])\n",
      "Targets:  torch.Size([1, 1024, 1024])\n",
      "Procssing image 0\n",
      "Procssing image 10\n",
      "Procssing image 20\n",
      "Procssing image 30\n",
      "Procssing image 40\n",
      "Procssing image 50\n",
      "Procssing image 60\n",
      "Procssing image 70\n",
      "Procssing image 80\n",
      "Procssing image 90\n",
      "Procssing image 100\n",
      "Procssing image 110\n",
      "Procssing image 120\n",
      "Procssing image 130\n",
      "Procssing image 140\n",
      "Procssing image 150\n",
      "Procssing image 160\n",
      "Procssing image 170\n",
      "Procssing image 180\n",
      "Procssing image 190\n",
      "Procssing image 200\n",
      "Procssing image 210\n",
      "Procssing image 220\n",
      "Procssing image 230\n",
      "Procssing image 240\n",
      "Procssing image 250\n",
      "Procssing image 260\n",
      "Procssing image 270\n",
      "Procssing image 350\n",
      "Procssing image 360\n",
      "Procssing image 370\n",
      "Procssing image 380\n",
      "Procssing image 390\n",
      "Procssing image 400\n",
      "Procssing image 410\n",
      "Procssing image 420\n",
      "Procssing image 430\n",
      "Procssing image 440\n",
      "Procssing image 450\n",
      "Procssing image 460\n",
      "Procssing image 470\n",
      "Procssing image 480\n",
      "Procssing image 490\n",
      "Procssing image 500\n",
      "Procssing image 510\n",
      "Procssing image 520\n",
      "Procssing image 530\n",
      "Procssing image 540\n",
      "Procssing image 550\n",
      "Procssing image 560\n",
      "Procssing image 570\n",
      "Procssing image 580\n",
      "Procssing image 590\n",
      "Procssing image 600\n",
      "Procssing image 610\n",
      "Procssing image 620\n",
      "Procssing image 630\n",
      "Procssing image 640\n",
      "Procssing image 650\n",
      "Procssing image 660\n",
      "Procssing image 670\n",
      "Procssing image 680\n",
      "Procssing image 690\n",
      "Procssing image 700\n",
      "Procssing image 710\n",
      "Procssing image 720\n",
      "Procssing image 730\n",
      "Procssing image 740\n",
      "Procssing image 750\n",
      "Procssing image 760\n",
      "Procssing image 770\n",
      "Procssing image 780\n",
      "Procssing image 790\n",
      "Procssing image 800\n",
      "Procssing image 810\n",
      "Procssing image 820\n",
      "Procssing image 830\n",
      "Procssing image 840\n",
      "Procssing image 850\n",
      "Procssing image 860\n",
      "Procssing image 870\n",
      "Procssing image 880\n",
      "Procssing image 890\n",
      "Procssing image 900\n",
      "Procssing image 910\n",
      "Procssing image 920\n",
      "Procssing image 930\n",
      "Procssing image 940\n",
      "Procssing image 950\n",
      "Procssing image 960\n",
      "Procssing image 970\n",
      "Procssing image 980\n",
      "Procssing image 990\n",
      "Procssing image 1000\n",
      "Procssing image 1010\n",
      "Procssing image 1020\n",
      "Procssing image 1030\n",
      "Procssing image 1040\n",
      "Procssing image 1050\n",
      "Procssing image 1060\n",
      "Procssing image 1070\n",
      "Procssing image 1080\n",
      "Procssing image 1090\n",
      "Procssing image 1100\n",
      "Procssing image 1110\n",
      "Procssing image 1120\n",
      "Procssing image 1130\n",
      "Procssing image 1140\n",
      "Procssing image 1150\n",
      "Procssing image 1160\n",
      "Procssing image 1170\n",
      "Procssing image 1180\n",
      "Procssing image 1190\n",
      "Procssing image 1200\n",
      "Procssing image 1210\n",
      "Procssing image 1220\n",
      "Procssing image 1230\n",
      "Procssing image 1240\n",
      "Procssing image 1250\n",
      "Procssing image 1260\n",
      "Procssing image 1270\n",
      "Procssing image 1280\n",
      "Procssing image 1290\n",
      "Procssing image 1300\n",
      "Procssing image 1310\n",
      "Procssing image 1320\n",
      "Procssing image 1330\n",
      "Procssing image 1340\n",
      "Procssing image 1350\n",
      "Procssing image 1360\n",
      "Procssing image 1370\n",
      "Procssing image 1380\n",
      "Procssing image 1390\n",
      "Procssing image 1400\n",
      "Procssing image 1410\n",
      "Procssing image 1420\n",
      "Procssing image 1430\n",
      "Procssing image 1440\n",
      "Procssing image 1450\n",
      "Procssing image 1460\n",
      "Procssing image 1470\n",
      "Procssing image 1480\n",
      "Procssing image 1490\n",
      "Procssing image 1500\n",
      "Procssing image 1510\n",
      "Procssing image 1520\n",
      "Procssing image 1530\n",
      "Procssing image 1540\n",
      "Procssing image 1550\n",
      "Procssing image 1560\n",
      "Procssing image 1570\n",
      "Procssing image 1580\n",
      "Procssing image 1590\n",
      "Procssing image 1600\n",
      "Procssing image 1610\n",
      "Procssing image 1620\n",
      "Procssing image 1630\n",
      "Procssing image 1640\n",
      "Procssing image 1650\n",
      "Procssing image 1660\n",
      "Procssing image 1670\n",
      "Procssing image 1680\n",
      "Procssing image 1690\n",
      "Procssing image 1700\n",
      "Procssing image 1710\n",
      "Procssing image 1720\n",
      "Procssing image 1730\n",
      "Procssing image 1740\n",
      "Procssing image 1750\n",
      "Procssing image 1760\n",
      "Procssing image 1770\n",
      "Procssing image 1780\n",
      "Procssing image 1790\n",
      "Procssing image 1800\n",
      "Procssing image 1810\n",
      "Procssing image 1820\n",
      "Procssing image 1830\n",
      "Procssing image 1840\n",
      "Procssing image 1850\n",
      "Procssing image 1860\n",
      "Procssing image 1870\n",
      "Procssing image 1880\n",
      "Procssing image 1890\n",
      "Procssing image 1900\n",
      "Procssing image 1910\n",
      "Procssing image 1920\n",
      "Procssing image 1930\n",
      "Procssing image 1940\n",
      "Procssing image 1950\n",
      "Procssing image 1960\n",
      "Procssing image 1970\n",
      "Procssing image 1980\n",
      "Procssing image 1990\n",
      "Procssing image 2000\n",
      "Procssing image 2010\n",
      "Procssing image 2020\n",
      "Procssing image 2030\n",
      "Procssing image 2040\n",
      "Procssing image 2050\n",
      "Procssing image 2060\n",
      "Procssing image 2070\n",
      "Procssing image 2080\n",
      "Procssing image 2090\n",
      "Procssing image 2100\n",
      "Procssing image 2110\n",
      "Procssing image 2120\n",
      "Procssing image 2130\n",
      "Procssing image 2140\n",
      "Procssing image 2150\n",
      "Procssing image 2160\n",
      "Procssing image 2170\n",
      "Procssing image 2180\n",
      "Procssing image 2190\n",
      "Procssing image 2200\n",
      "Procssing image 2210\n",
      "Procssing image 2220\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Train: %d\" %len(train_loader.dataset))\n",
    "print(\"Val: %d\" %len(val_loader.dataset.imgs))\n",
    "print(\"Test: %d\" %len(test_loader.dataset.imgs))\n",
    "# print(\"Classes: %d\" % len(train_loader.dataset.classes))\n",
    "\n",
    "print((iter(train_loader)))\n",
    "\n",
    "inputs, targets = next(iter(train_loader))\n",
    "print(\"Inputs: \", inputs.size())\n",
    "print(\"Targets: \", targets.size())\n",
    "\n",
    "# utils.imgs.view_image(inputs[0])\n",
    "# utils.imgs.view_image(targets[0])\n",
    "# utils.imgs.view_annotated(targets[0])\n",
    "\n",
    "# print(targets[0])\n",
    "\n",
    "\n",
    "\n",
    "for i,(image,label) in enumerate(iter(test_loader)):\n",
    "    if i % 10 == 0:\n",
    "        print(\"Procssing image\",i)\n",
    "        \n",
    "    im = image[0]\n",
    "    \n",
    "    # scale to [0,1]\n",
    "    im -= im.min()\n",
    "    im /= im.max()\n",
    "    \n",
    "    im = torchvision.transforms.ToPILImage()(im)\n",
    "    im.save(\"ds_test/\" + str(i) + \".png\")\n",
    "    \n",
    "    label = label.float()\n",
    "    la = torchvision.transforms.ToPILImage()(label)\n",
    "    la.save(\"ds_test/\" + str(i) + \".mask.png\")\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
