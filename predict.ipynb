{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!jupyter notebook list\n",
    "%env CUDA_VISIBLE_DEVICES=3\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import tiramisu\n",
    "from models import tiramisu_bilinear\n",
    "from models import tiramisu_m1\n",
    "from datasets import deepglobe\n",
    "from datasets import maroads\n",
    "from datasets import joint_transforms\n",
    "import utils.imgs\n",
    "import utils.training as train_utils\n",
    "\n",
    "# tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CamVid\n",
    "\n",
    "Clone this repository which holds the CamVid dataset\n",
    "```\n",
    "git clone https://github.com/alexgkendall/SegNet-Tutorial\n",
    "```\n",
    "No. Place deepglobe dataset in datasets/deepglobe/dataset/train,test,valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = \"expM.1.drop2.1.dicebce\"\n",
    "out_run = \"\"\n",
    "DEEPGLOBE_PATH = Path('datasets/', 'deepglobe/dataset')\n",
    "MAROADS_PATH = Path('datasets/', 'maroads/dataset')\n",
    "RESULTS_PATH = Path('.results/')\n",
    "WEIGHTS_PATH = Path('.weights/') / run\n",
    "RUNS_PATH    = Path('.runs/')\n",
    "RESULTS_PATH.mkdir(exist_ok=True)\n",
    "WEIGHTS_PATH.mkdir(exist_ok=True)\n",
    "RUNS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "batch_size = 1 # TODO: Should be `MAX_BATCH_PER_CARD * torch.cuda.device_count()` (which in this case is 1 assuming max of 1 batch per card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize = joint_transforms.JointRandomCrop((300, 300))\n",
    "\n",
    "normalize = transforms.Normalize(mean=deepglobe.mean, std=deepglobe.std)\n",
    "train_joint_transformer = transforms.Compose([\n",
    "#     resize,\n",
    "    joint_transforms.JointRandomHorizontalFlip(),\n",
    "    joint_transforms.JointRandomVerticalFlip(),\n",
    "    joint_transforms.JointRandomRotate()\n",
    "    ])\n",
    "\n",
    "train_slice = slice(None,4000)\n",
    "test_slice = slice(4000,None)\n",
    "\n",
    "train_dset = deepglobe.DeepGlobe(DEEPGLOBE_PATH, 'train', slc = train_slice,\n",
    "    joint_transform=train_joint_transformer,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=.4,contrast=.4,saturation=.4),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "train_dset_ma = maroads.MARoads(MAROADS_PATH, \n",
    "    joint_transform=train_joint_transformer,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=.4,contrast=.4,saturation=.4),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "# print(len(train_dset_ma.imgs))\n",
    "# print(len(train_dset_ma.msks))\n",
    "train_dset_combine = torch.utils.data.ConcatDataset((train_dset, train_dset_ma))\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dset_ma, batch_size=batch_size, shuffle=True)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dset_combine, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# resize_joint_transformer = transforms.Compose([\n",
    "#     resize\n",
    "#     ])\n",
    "resize_joint_transformer = None\n",
    "val_dset = deepglobe.DeepGlobe(\n",
    "    DEEPGLOBE_PATH, 'valid', joint_transform=resize_joint_transformer,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]))\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dset = deepglobe.DeepGlobe(\n",
    "    DEEPGLOBE_PATH, 'train', joint_transform=resize_joint_transformer, slc = test_slice,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]))\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train: %d\" %len(train_loader.dataset))\n",
    "print(\"Val: %d\" %len(val_loader.dataset.imgs))\n",
    "print(\"Test: %d\" %len(test_loader.dataset.imgs))\n",
    "# print(\"Classes: %d\" % len(train_loader.dataset.classes))\n",
    "\n",
    "print((iter(train_loader)))\n",
    "\n",
    "inputs, targets = next(iter(train_loader))\n",
    "print(\"Inputs: \", inputs.size())\n",
    "print(\"Targets: \", targets.size())\n",
    "\n",
    "utils.imgs.view_image(inputs[0])\n",
    "# utils.imgs.view_image(targets[0])\n",
    "utils.imgs.view_annotated(targets[0])\n",
    "\n",
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "LR_DECAY = 0.995\n",
    "DECAY_EVERY_N_EPOCHS = 1\n",
    "N_EPOCHS = 1000\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bceloss import dice_bce_loss\n",
    "from loss.BCESSIM import BCESSIM\n",
    "\n",
    "model = tiramisu_m1.FCDenseNetSmall(n_classes=1, dropout_rate=0.2).cuda()\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "\n",
    "criterion = dice_bce_loss()\n",
    "# criterion = BCESSIM()\n",
    "\n",
    "# summary(model, input_size=inputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "# start_epoch = train_utils.load_weights(model, (WEIGHTS_PATH/'latest.th'))\n",
    "start_epoch = train_utils.load_weights(model, (WEIGHTS_PATH/'weights-368-1.008-0.462.pth'))\n",
    "print(start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Writer will output to ./runs/ directory by default\n",
    "# run = \"26\"\n",
    "# writer = SummaryWriter(log_dir=(\"./.runs/run\" + str(run) + \"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save images\n",
    "from utils import imgs as img_utils\n",
    "\n",
    "\n",
    "OUT_PATH = Path('out/') / (run + out_run)\n",
    "OUT_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "predictions = train_utils.predict_validation(model, val_loader)\n",
    "\n",
    "\n",
    "for pred in predictions:\n",
    "\n",
    "    im = transforms.ToPILImage()(pred[1]*255).convert(\"RGB\")\n",
    "    print(pred[2][0])\n",
    "    path = os.path.join(OUT_PATH,os.path.split(pred[2][0])[1])\n",
    "    print(path)\n",
    "    im.save(path,\"PNG\")\n",
    "    \n",
    "_pred = predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "break\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# break # errors. Used to stop \"run all\"\n",
    "for epoch in range(start_epoch, N_EPOCHS+1):\n",
    "    since = time.time()\n",
    "\n",
    "    ### Train ###\n",
    "    trn_loss, trn_err = train_utils.train(\n",
    "        model, train_loader, optimizer, criterion, epoch)\n",
    "    print('Epoch {:d}\\nTrain - Loss: {:.4f}, Acc: {:.4f}'.format(\n",
    "        epoch, trn_loss, 1-trn_err))    \n",
    "    time_elapsed = time.time() - since  \n",
    "    print('Train Time {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "#     ### Test ###\n",
    "#     val_loss, val_err = train_utils.test(model, val_loader, criterion, epoch)    \n",
    "#     print('Val - Loss: {:.4f} | Acc: {:.4f}'.format(val_loss, 1-val_err))\n",
    "#     time_elapsed = time.time() - since  \n",
    "#     print('Total Time {:.0f}m {:.0f}s\\n'.format(\n",
    "#         time_elapsed // 60, time_elapsed % 60))\n",
    "#     val_loss = trn_loss\n",
    "#     val_err = trn_err\n",
    "    ### Test ###\n",
    "    tes_loss, tes_err, tes_iou = train_utils.test(model, test_loader, criterion, epoch)    \n",
    "    print('Tes - Loss: {:.4f} | Acc: {:.4f}'.format(tes_loss, 1-tes_err))\n",
    "    time_elapsed = time.time() - since  \n",
    "    print('Total Time {:.0f}m {:.0f}s\\n'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "#     val_loss = trn_loss\n",
    "#     val_err = trn_err\n",
    "    \n",
    "    ### Checkpoint ###    \n",
    "    train_utils.save_weights(model, epoch, tes_loss, tes_err)\n",
    "\n",
    "    ### Adjust Lr ###\n",
    "#     train_utils.adjust_learning_rate(LR, LR_DECAY, optimizer, \n",
    "#                                      epoch, DECAY_EVERY_N_EPOCHS)\n",
    "    \n",
    "    # Log on tensorboard\n",
    "    writer.add_scalar('Loss/train', trn_loss, epoch)\n",
    "    writer.add_scalar('Loss/test', tes_loss, epoch)\n",
    "    \n",
    "    writer.add_scalar('Error/train', trn_err, epoch)\n",
    "    writer.add_scalar('Error/test', tes_err, epoch)\n",
    "    \n",
    "#     writer.add_scalar('Accuracy/train', trn_iou, epoch)\n",
    "    writer.add_scalar('Accuracy/test', tes_iou, epoch)\n",
    "    \n",
    "#     writer.add_scalar('Accuracy/train', epoch_acc, epoch)\n",
    "#     writer.add_scalar('Accuracy/test/noaug', do_valid(False), epoch)\n",
    "#     writer.add_scalar('Accuracy/test/tta', do_valid(True), epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        writer.add_scalar('Params/learning_rage', param_group['lr'], epoch)\n",
    "#     writer.add_scalar('params/learning_rate', optimizer.lr, epoch)\n",
    "#     writer.add_scalar('Params/no_optim', no_optim, epoch)\n",
    "\n",
    "    # show a sample image\n",
    "    for i in range(3):\n",
    "        inputs, targets, pred, loss, err, iou = train_utils.get_sample_predictions(model, test_loader, n=1, criterion=criterion)\n",
    "#         print(inputs.shape)\n",
    "        raw = model(inputs.cuda()).cpu()\n",
    "#         print(raw.shape)\n",
    "#         print(pred.shape)\n",
    "\n",
    "# #         img = pred\n",
    "        \n",
    "# #         img = torchvision.utils.make_grid([inputs, targets, pred], nrow=8, padding=2, normalize=False, range=None, scale_each=False, pad_value=0)\n",
    "        \n",
    "# #         img = torchvision.utils.make_grid(torch.stack([inputs[0], targets[0], pred[0].float().float()]))\n",
    "# #         print(inputs.shape)\n",
    "# #         print(targets.shape)\n",
    "# #         print(pred.shape)\n",
    "        \n",
    "#         # print stats on raw\n",
    "#         print(\"max\", raw.max())\n",
    "#         print(\"min\", raw.min())\n",
    "        \n",
    "        \n",
    "        img = torchvision.utils.make_grid(torch.stack([\n",
    "            inputs[0],\n",
    "            targets[0].unsqueeze(0).expand(3,-1,-1).float(), \n",
    "            pred[0].unsqueeze(0).expand(3,-1,-1).float(),\n",
    "            raw[0].expand(3,-1,-1).float()\n",
    "        ]), normalize=True)\n",
    "        \n",
    "\n",
    "        writer.add_image('test/sample_pred', img, epoch)\n",
    "        break\n",
    "    \n",
    "    start_epoch = epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns test_loss, test_error, jaccard\n",
    "train_utils.test(model, test_loader, criterion, epoch=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_utils.test(model, test_loader, criterion, epoch=1, use_tta=True)  # flip tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_utils.test(model, test_loader, criterion, epoch=1, use_tta=True)  # rot tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = train_utils.view_sample_predictions(model, test_loader, n=1, criterion=criterion)\n",
    "print(\"loss\", \"error\", \"jaccard\")\n",
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
