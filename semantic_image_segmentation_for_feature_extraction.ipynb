{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2 semantic image segmentation for feature extraction",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhSUg2ISQFw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat /etc/os-release\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE-lNGdNbmJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "from torchvision import models\n",
        "import torchvision\n",
        "  \n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import math\n",
        "import numbers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Get dataset\n",
        "use_gdrive = False\n",
        "if (use_gdrive):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/', True)\n",
        "  dataset_dir = \"/content/drive/My Drive/MIIPS/Deepglobe/road-train-1.v2/\"\n",
        "  project_dir = \"/content/drive/My Drive/MIIPS/Deepglobe/road-train-1.v2/\"\n",
        "else:\n",
        "  project_dir = \"/opt/colab/Road_extraction/\"\n",
        "  dataset_dir = project_dir + \"dataset/\"\n",
        "\n",
        "  \n",
        "  \n",
        "os.chdir(project_dir)\n",
        "\n",
        "model_save_path = \"classifier.pth\"\n",
        "\n",
        "\n",
        "!ls {dataset_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PELfQmSuoLDT",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "43Uhrbl8FZ9P",
        "colab": {}
      },
      "source": [
        "# some flags\n",
        "USE_FULL_SIZE_IMAGES = True\n",
        "\n",
        "\n",
        "# Define network\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, padding=1, kernel_size=3, stride=1, with_nonlinearity=True):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, padding=padding, kernel_size=kernel_size, stride=stride)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.with_nonlinearity = with_nonlinearity\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        if self.with_nonlinearity:\n",
        "            x = self.relu(x)\n",
        "        return x  \n",
        "      \n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        \n",
        "        # Following Decoder Block in Figure 2\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
        "            ConvBlock(out_channels,out_channels),\n",
        "            ConvBlock(out_channels,out_channels)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "      \n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, n_classes=1, num_filters=32, pretrained=True, is_deconv=False, dropout_p=0.3):\n",
        "        \"\"\"\n",
        "        :param n_classes:\n",
        "        :param num_filters:\n",
        "        :param pretrained:\n",
        "            False - no pre-trained network is used\n",
        "            True  - encoder is pre-trained with resnet34\n",
        "        :is_deconv:\n",
        "            False: bilinear interpolation is used in decoder\n",
        "            True: deconvolution is used in decoder\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_classes = n_classes\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.encoder = torchvision.models.resnet34(pretrained=pretrained)\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv1 = nn.Sequential(self.encoder.conv1,\n",
        "                                   self.encoder.bn1,\n",
        "                                   self.encoder.relu,\n",
        "                                   self.pool)\n",
        "        self.conv2 = self.encoder.layer1\n",
        "        self.conv3 = self.encoder.layer2\n",
        "        self.conv4 = self.encoder.layer3\n",
        "        self.conv5 = self.encoder.layer4\n",
        "        \n",
        "        self.center = DecoderBlock(512, 256) # self.center is the decoder right after the pool 2x2\n",
        "        \n",
        "        self.dec5 = DecoderBlock(768, 256)\n",
        "        self.dec4 = DecoderBlock(512, 256)\n",
        "        self.dec3 = DecoderBlock(384, 64)\n",
        "        self.dec2 = DecoderBlock(128, 128)\n",
        "        self.dec1 = DecoderBlock(128, 32)\n",
        "        self.dec0 = ConvBlock(32, 32)\n",
        "        self.final = nn.Conv2d(32, n_classes, kernel_size=1)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.conv1(x)\n",
        "        conv2 = self.conv2(conv1)\n",
        "        conv3 = self.conv3(conv2)\n",
        "        conv4 = self.conv4(conv3)\n",
        "        conv5 = self.conv5(conv4)\n",
        "\n",
        "        center = self.center(self.pool(conv5))\n",
        "\n",
        "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
        "\n",
        "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
        "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
        "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
        "        dec1 = self.dec1(dec2)\n",
        "        dec0 = self.dec0(dec1)\n",
        "\n",
        "        if self.n_classes > 1:\n",
        "            x_out = F.log_softmax(self.final(dec0), dim=1)\n",
        "        else:\n",
        "            x_out = sigmoid(self.final(dec0))\n",
        "\n",
        "#         return self.dropout(x_out)\n",
        "        return x_out\n",
        "\n",
        "\n",
        "# Define loss/acc functions\n",
        "from torch.nn import Module\n",
        "from torch import sigmoid\n",
        "\n",
        "# Matches standard definition of jaccard (and definition on codelab)  \n",
        "def jaccard(outputs, targets):\n",
        "  outputs = outputs.round().int()\n",
        "  targets = targets.round().int()\n",
        "\n",
        "  intersection = outputs & targets\n",
        "  union = outputs | targets\n",
        "\n",
        "  return float(intersection.sum())/float(union.sum())\n",
        "\n",
        "# From previous implementation\n",
        "def soft_jaccard(outputs, targets, weight=1):\n",
        "    eps = 1e-15\n",
        "    jaccard_target = (targets == 1).float()\n",
        "    jaccard_output = sigmoid(outputs)\n",
        "\n",
        "    intersection = (jaccard_output * jaccard_target).sum()\n",
        "    union = jaccard_output.sum() + jaccard_target.sum()\n",
        "    return intersection / (union - intersection + eps)\n",
        "\n",
        "    \n",
        "# Directly from the paper     \n",
        "def paper_jaccard(predictions, labels):\n",
        "  predictions = predictions.reshape(-1)\n",
        "  labels = labels.reshape(-1)\n",
        "\n",
        "  summ = (labels*predictions)/(labels + predictions - labels*predictions)\n",
        "  \n",
        "  return summ.sum()/len(predictions);\n",
        "\n",
        "\n",
        "# Directly from the paper\n",
        "class PaperLoss:\n",
        "  def __init__(self, alpha = 0.7):\n",
        "    self.alpha = alpha\n",
        "    self.bce = nn.BCELoss()\n",
        "    \n",
        "  def __call__(self, predictions, labels):\n",
        "    j = paper_jaccard(predictions, labels)\n",
        "    loss = self.alpha*self.bce(predictions,labels) - (1 - self.alpha)*math.log(j)\n",
        "    return loss\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Dataset utils    \n",
        "class SeedableRandomRotation(object):\n",
        "    \"\"\"Rotate the image by angle.\n",
        "       Adapted version of RandomRotation (https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomRotation) that allows for a specified seed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, degrees, resample=False, expand=False, center=None, seed = random.random()):\n",
        "        if isinstance(degrees, numbers.Number):\n",
        "            if degrees < 0:\n",
        "                raise ValueError(\"If degrees is a single number, it must be positive.\")\n",
        "            self.degrees = (-degrees, degrees)\n",
        "        else:\n",
        "            if len(degrees) != 2:\n",
        "                raise ValueError(\"If degrees is a sequence, it must be of len 2.\")\n",
        "            self.degrees = degrees\n",
        "\n",
        "        self.resample = resample\n",
        "        self.expand = expand\n",
        "        self.center = center\n",
        "        self.random = random.Random()\n",
        "        self.random.seed(seed)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(degrees, rand = random):\n",
        "        \"\"\"Get parameters for ``rotate`` for a random rotation.\n",
        "\n",
        "        Returns:\n",
        "            sequence: params to be passed to ``rotate`` for random rotation.\n",
        "        \"\"\"\n",
        "        angle = rand.uniform(degrees[0], degrees[1])\n",
        "\n",
        "        return angle\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "            img (PIL Image): Image to be rotated.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image: Rotated image.\n",
        "        \"\"\"\n",
        "\n",
        "        angle = self.get_params(self.degrees, rand = self.random)\n",
        "\n",
        "        return transforms.functional.rotate(img, angle, self.resample, self.expand, self.center)\n",
        "\n",
        "\n",
        "class SeedableRandomResizedCrop(object):\n",
        "    \"\"\"Crop the given PIL Image to random size and aspect ratio.\n",
        "       Adapted version of RandomResizedCrop (https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomResizedCrop) that allows for a specified seed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.), interpolation=Image.BILINEAR, seed = random.random()):\n",
        "        if isinstance(size, tuple):\n",
        "            self.size = size\n",
        "        else:\n",
        "            self.size = (size, size)\n",
        "        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):\n",
        "            warnings.warn(\"range should be of kind (min, max)\")\n",
        "\n",
        "        self.interpolation = interpolation\n",
        "        self.scale = scale\n",
        "        self.ratio = ratio\n",
        "        self.random = random.Random()\n",
        "        self.random.seed(seed)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(img, scale, ratio, rand = random):\n",
        "        \"\"\"Get parameters for ``crop`` for a random sized crop.\n",
        "\n",
        "        Args:\n",
        "            img (PIL Image): Image to be cropped.\n",
        "            scale (tuple): range of size of the origin size cropped\n",
        "            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n",
        "\n",
        "        Returns:\n",
        "            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n",
        "                sized crop.\n",
        "        \"\"\"\n",
        "        area = img.size[0] * img.size[1]\n",
        "\n",
        "        for attempt in range(10):\n",
        "            target_area = rand.uniform(*scale) * area\n",
        "            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))\n",
        "            aspect_ratio = math.exp(rand.uniform(*log_ratio))\n",
        "\n",
        "            w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
        "            h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
        "\n",
        "            if w <= img.size[0] and h <= img.size[1]:\n",
        "                i = rand.randint(0, img.size[1] - h)\n",
        "                j = rand.randint(0, img.size[0] - w)\n",
        "                return i, j, h, w\n",
        "\n",
        "        # Fallback to central crop\n",
        "        in_ratio = img.size[0] / img.size[1]\n",
        "        if (in_ratio < min(ratio)):\n",
        "            w = img.size[0]\n",
        "            h = w / min(ratio)\n",
        "        elif (in_ratio > max(ratio)):\n",
        "            h = img.size[1]\n",
        "            w = h * max(ratio)\n",
        "        else:  # whole image\n",
        "            w = img.size[0]\n",
        "            h = img.size[1]\n",
        "        i = (img.size[1] - h) // 2\n",
        "        j = (img.size[0] - w) // 2\n",
        "        return i, j, h, w\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image): Image to be cropped and resized.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image: Randomly cropped and resized image.\n",
        "        \"\"\"\n",
        "        i, j, h, w = self.get_params(img, self.scale, self.ratio, rand = self.random)\n",
        "        return transforms.functional.resized_crop(img, i, j, h, w, self.size, self.interpolation)\n",
        "\n",
        "    \n",
        "    \n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, image_paths, target_paths):\n",
        "      \n",
        "      assert(len(image_paths) == len(target_paths))  # sloppy sanity check\n",
        "      \n",
        "      self.image_paths = image_paths\n",
        "      self.target_paths = target_paths\n",
        "      \n",
        "    def __getitem__(self, index):\n",
        "        image = Image.open(self.image_paths[index])\n",
        "        mask = Image.open(self.target_paths[index]).convert('1')\n",
        "        \n",
        "        \n",
        "        randomSeed = random.random()  # Ensure both the image and mask have the same seed so that the same transformations are applied to both\n",
        "        \n",
        "        if (USE_FULL_SIZE_IMAGES):\n",
        "          # Don't include random crop\n",
        "          # TODO: There's probably a cleaner way to do this\n",
        "          t_image = transforms.Compose([\n",
        "              transforms.ColorJitter(brightness=0.2,contrast=0.2,hue=0.02),\n",
        "  #             SeedableRandomResizedCrop(size = 448, scale = (0.6, 1.4), seed = randomSeed),\n",
        "              SeedableRandomRotation(degrees = 30, seed = randomSeed),\n",
        "              transforms.ToTensor()\n",
        "          ])(image)\n",
        "          t_mask = transforms.Compose([\n",
        "  #             SeedableRandomResizedCrop(size = 448, scale = (0.6, 1.4), seed = randomSeed),\n",
        "              SeedableRandomRotation(degrees = 30, seed = randomSeed),\n",
        "              transforms.ToTensor()\n",
        "          ])(mask)\n",
        "        else:\n",
        "          t_image = transforms.Compose([\n",
        "              transforms.ColorJitter(brightness=0.2,contrast=0.2,hue=0.02),\n",
        "              SeedableRandomResizedCrop(size = 448, scale = (0.6, 1.4), seed = randomSeed),\n",
        "              SeedableRandomRotation(degrees = 30, seed = randomSeed),\n",
        "              transforms.ToTensor()\n",
        "          ])(image)\n",
        "          t_mask = transforms.Compose([\n",
        "              SeedableRandomResizedCrop(size = 448, scale = (0.6, 1.4), seed = randomSeed),\n",
        "              SeedableRandomRotation(degrees = 30, seed = randomSeed),\n",
        "              transforms.ToTensor()\n",
        "          ])(mask)\n",
        "\n",
        "        \n",
        "        return t_image, t_mask\n",
        "    \n",
        "    def __len__(self): \n",
        "        return len(self.image_paths)\n",
        "\n",
        "      \n",
        "      \n",
        "# Load dataset      \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "folder_data = glob.glob(dataset_dir + \"train/*.jpg\")\n",
        "folder_mask = glob.glob(dataset_dir + \"train/*.png\")\n",
        "\n",
        "len_data = len(folder_data)\n",
        "train_size = 0.7\n",
        "\n",
        "\n",
        "# Sort because glob returns in a nondeterministic order\n",
        "folder_data = sorted(folder_data)\n",
        "folder_mask = sorted(folder_mask)\n",
        "\n",
        "train_image_paths = folder_data[:int(len_data*train_size)]\n",
        "test_image_paths = folder_data[int(len_data*train_size):]\n",
        "\n",
        "train_mask_paths = folder_mask[:int(len_data*train_size)]\n",
        "test_mask_paths = folder_mask[int(len_data*train_size):]\n",
        "\n",
        "train_dataset = CustomDataset(train_image_paths, train_mask_paths)\n",
        "test_dataset = CustomDataset(test_image_paths, test_mask_paths)\n",
        "\n",
        "print(\"Size of train dataset :\", len(train_dataset))\n",
        "print(\"Size of test dataset  :\", len(test_dataset))\n",
        "print(\"Total number of images:\", len(train_dataset) + len(test_dataset))\n",
        "\n",
        "batch_size = 8\n",
        "if (USE_FULL_SIZE_IMAGES):\n",
        "  # Full size images take up a lot of memory; decrease batch size so that it fits\n",
        "  batch_size = 2\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# Load model\n",
        "model = UNet(n_classes=1).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-04, weight_decay = 1e-04)\n",
        "\n",
        "\n",
        "criterion = PaperLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7PS-gZZc7Zy",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irv7fWvIbk4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "print(\"Started at:\", datetime.datetime.now())\n",
        "\n",
        "for epoch in range(35):  # loop over the dataset multiple times (about 20k batches total)\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for i, (image, mask) in enumerate(train_loader):\n",
        "        # get the inputs\n",
        "        inputs, mask = image.to(torch.cuda.current_device()), mask.to(torch.cuda.current_device())\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, mask)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:    # print every 20 mini-batches\n",
        "          print('[%d, %5d] loss: %.3f, accuracy: %.3f'  % (epoch + 1, i + 1, running_loss/20, jaccard (outputs, mask)))\n",
        "          running_loss = 0.0\n",
        "          \n",
        "        \n",
        "        if i % 100 == 99:  # save every 100 mini-batches\n",
        "          torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print('Saving')\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print('Finished Training')\n",
        "print(\"Finished at:\", datetime.datetime.now())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ9JhSm1c-Ce",
        "colab_type": "text"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xeb0JysiaLED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tta_output(inputs, model):\n",
        "  '''\n",
        "  Makes predictions on 4 rotations of the inputs and averages them\n",
        "  '''\n",
        "  \n",
        "  outputs = model(inputs)\n",
        "\n",
        "  rots = 4\n",
        "\n",
        "  # Predict non-rotated image\n",
        "  outputs = model(inputs)\n",
        "\n",
        "  # Predict the rotated images\n",
        "  for rot in range(1, rots):\n",
        "\n",
        "    # Incredibly slow way to do it, defeats the purpose of using gpu\n",
        "    inputs_rot = torch.stack([\n",
        "        transforms.ToTensor()(\n",
        "        transforms.functional.rotate(\n",
        "            transforms.ToPILImage()(inpt.cpu()),\n",
        "            90*rot)\n",
        "        ).to(torch.cuda.current_device())\n",
        "\n",
        "        for inpt in inputs\n",
        "    ])\n",
        "\n",
        "    outputs_rot = model(inputs_rot)\n",
        "\n",
        "    # Unrotate and add to the running average\n",
        "    outputs += torch.stack([\n",
        "        transforms.ToTensor()(\n",
        "        transforms.functional.rotate(\n",
        "            transforms.ToPILImage()(otpt.cpu()),\n",
        "            -90*rot)\n",
        "        ).to(torch.cuda.current_device())\n",
        "\n",
        "        for otpt in outputs_rot\n",
        "    ])\n",
        "\n",
        "\n",
        "  # Taking an average, so divide by the number of rotations\n",
        "  outputs /= rots\n",
        "  \n",
        "  return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY_w7fZsc6jA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "debug_print_every = 1\n",
        "USE_TTA = False\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "acc_fn = jaccard\n",
        "\n",
        "\n",
        "model = UNet().to(device)\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "\n",
        "print(\"Done loading model.\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "running_loss = 0.0\n",
        "running_acc = 0.0\n",
        "total_acc = 0.0\n",
        "  \n",
        "print( datetime.datetime.now(),\"Starting validation\",)  \n",
        "with torch.no_grad():\n",
        "      for i, (image, mask) in enumerate(test_loader):\n",
        "        inputs, mask = image.to(torch.cuda.current_device()), mask.to(torch.cuda.current_device())\n",
        "\n",
        "        if USE_TTA:\n",
        "          outputs = get_tta_output(inputs, model)\n",
        "        else:\n",
        "          outputs = model(inputs)\n",
        "          \n",
        "        loss = criterion(outputs, mask)\n",
        "        \n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        running_acc += acc_fn(outputs, mask)\n",
        "        total_acc += acc_fn(outputs, mask)\n",
        "        \n",
        "        if i % debug_print_every == (debug_print_every-1):    # print every 20 mini-batches\n",
        "          print('[%5d] loss: %.3f,\\taccuracy: %.3f,\\tavg_acc: %.3f'  % (i + 1, running_loss/debug_print_every, running_acc/debug_print_every, total_acc/(i+1)))\n",
        "          \n",
        "          running_loss = 0.0\n",
        "          running_acc = 0.0\n",
        "\n",
        "print(datetime.datetime.now(), 'Finished validation with average accuracy:', total_acc/(i+1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4U2SYLMEXGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display validation results\n",
        "max_iters = 200\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "model = UNet().to(device)\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "model.eval()\n",
        "print(\"Done loading.\")\n",
        "\n",
        "print(\"\\tInput\\tMask\\tprediction\\tbinarized\") # print headers\n",
        "with torch.no_grad():\n",
        "  for i, (image, mask) in enumerate(test_loader):\n",
        "    \n",
        "    if i >= max_iters:\n",
        "      break\n",
        "    \n",
        "    inputs, mask = image.to(torch.cuda.current_device()), mask.to(torch.cuda.current_device())\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, mask)\n",
        "    \n",
        "    print('[%d, %5d] loss: %.3f, accuracy: %.3f'  % (-1, i + 1, loss, jaccard (outputs, mask)))\n",
        "\n",
        "    \n",
        "    cmap = \"hot\"\n",
        "    cmap = \"nipy_spectral\"\n",
        "    cmap = \"viridis\" # default\n",
        "    \n",
        "    fig=plt.figure(figsize=(8, 8))\n",
        "    columns = 4\n",
        "    rows = 1\n",
        "    \n",
        "    fig.add_subplot(rows, columns, 1)\n",
        "    inputplot = plt.imshow(inputs.cpu()[0][0], cmap=cmap)\n",
        "    fig.add_subplot(rows, columns, 2)\n",
        "    maskplot = plt.imshow(mask.cpu()[0][0], cmap=cmap)\n",
        "    fig.add_subplot(rows, columns, 3)\n",
        "    outplot = plt.imshow(outputs.cpu()[0][0], cmap=cmap)\n",
        "    fig.add_subplot(rows, columns, 4)\n",
        "    outplot = plt.imshow(outputs.round().cpu()[0][0], cmap=cmap)    \n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK37eO4pg_A9",
        "colab_type": "text"
      },
      "source": [
        "# Predict on validation images (for submission)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCbtPHUFQxKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USE_TTA = False\n",
        "debug_show_images = False\n",
        "SAVE_IMAGES = True\n",
        "save_path = project_dir + \"out/\"\n",
        "\n",
        "if USE_TTA:\n",
        "#   threshold_bias = 0.25\n",
        "  threshold_bias = 0\n",
        "else:\n",
        "  threshold_bias = 0  # bias of 0 means thresholding at 0.5\n",
        "\n",
        "\n",
        "try:\n",
        "  os.mkdir(save_path)\n",
        "except FileExistsError:\n",
        "  print(\"Skipped creating \" + save_path + \"; already exists\")\n",
        "\n",
        "def binarize_and_save_mask():\n",
        "  pass\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import glob\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "image_paths = glob.glob(dataset_dir + \"valid/*.jpg\")\n",
        "\n",
        "model = UNet().to(device)\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "\n",
        "\n",
        "print(\"Done loading.\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i,path in enumerate(image_paths):\n",
        "    fileid = os.path.split(path)[1][:-len('_sat.jpg')]\n",
        "    print(fileid)\n",
        "\n",
        "    image = Image.open(path)\n",
        "\n",
        "    inputs = transforms.ToTensor()(image).unsqueeze(0).to(torch.cuda.current_device())\n",
        "    \n",
        "    if USE_TTA:\n",
        "      outputs = get_tta_output(inputs, model)\n",
        "    else:\n",
        "      outputs = model(inputs)\n",
        "\n",
        "\n",
        "    if debug_show_images:\n",
        "      cmap = \"viridis\" # default\n",
        "\n",
        "      fig=plt.figure(figsize=(8, 8))\n",
        "      columns = 3\n",
        "      rows = 1\n",
        "\n",
        "      fig.add_subplot(rows, columns, 1)\n",
        "      inputplot = plt.imshow(inputs.cpu()[0][0], cmap=cmap)\n",
        "      fig.add_subplot(rows, columns, 2)\n",
        "      outplot = plt.imshow(outputs.cpu()[0][0], cmap=cmap)\n",
        "      \n",
        "      # binarize\n",
        "      fig.add_subplot(rows, columns, 3)\n",
        "      outplot = plt.imshow((outputs + threshold_bias).round().cpu()[0][0], cmap=cmap)\n",
        "\n",
        "      plt.show()\n",
        "      \n",
        "    if SAVE_IMAGES:\n",
        "      outputs = outputs[0] # select 1st (and only) image from batch\n",
        "      outputs += threshold_bias  # bias so binarize threshold is changed\n",
        "      outputs = outputs.expand(3,-1,-1) # turn the grayscale image into 3 color channels\n",
        "      outputs = outputs.round()  # binarize (threshold of 0.5) # todo: this should be above expand, right? Or does it matter if it's just a view?\n",
        "      outputs = outputs*255      # scale to 0-255\n",
        "      out_img = outputs.cpu()  \n",
        "      \n",
        "      out_path = save_path + fileid + \"_mask.png\"\n",
        "      \n",
        "      torchvision.utils.save_image(out_img, out_path)\n",
        "      \n",
        "    print(\"{0:.2f} percent complete\".format((i+1)/len(image_paths)*100))\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}